**Ways of replication**

There are basically two different ways of replication, meaning replication can happen via two ways or two topologies
ie,

1) Leader Led replication
2) Leader Less replication


1) **Leader Led replication**

As the name suggest leader led so in the database cluster on node act as leader which will always take the
writes while others are followers. Mongo db, PostgreSQL, MySQL etc are databases which uses leader led replication.

Leader led replication can also be further divided into Leader follower and MultiLeader.

a) **Leader Follower**

Leader Follower replication as name suggests has a primary or a leader and then secondaries or slaves. So suppose in my database
cluster, I have in total 3 nodes , so one acts as a primary or master say P1, where other two nodes acts as secondary or slaves
say S1 and S2.


                                             --------------------- S1 (database node)
           Server-----------            P1(database node)
                                              ---------------------S2 (database node)

Now slaves will contains replica of master, and say we configure our system such that all write queries like insert, update, deletion
queries will be handled by primary and the slaves or secondary nodes will update replicas of the primary so that they are also
up to date and will server all kinds of read request.

Now lets understand the inner working that how these slaves will be able to catch up with the master node.

Say an insert, update or delete query is hosted by the primary node, so before sending acknowledgment to client it will
insert it into a replication log. These logs are inserted in a increment id say 1,2,3,4......

Now every replica or slave has a background thread working which is reading the logs and updating itself and keeping track of the updates
via this incremental ID, so say the replica log generated by primary at this point of time has 10 id's.

Now the s1 secondary started going through replica log and has updated itself till id 5. So it knows from where it need to start now
i,e ahead of 5. Same applies for other secondaries as well. Secondary periodically update primary about their catch up status.

**Benefits of replica logs**

1) These replication logs are generally kept static, lets suppose a insert query came in inside which the client said to insert
time stamp in time stamp column as current time. Now suppose P1 or master resolved this query and inserted time stamp T1 in its record.

Now instead of logging a static query in which the primary logged time stamp to be logged as T1 it inserted a
dynamic query which it received i,e insert time stamp in time stamp column as current time, then the secondary while replication
will insert their current time at which they executed this query and thus data inconsistency amongst the master and slaves will appear
and thus replica logs are kept static.

2) These replica logs can be used in Point in time recovery of the nodes.

Suppose we are allowing backup of our data at every 6 hours so consider the data being backed up at


12:00 AM---------------------> 6:00 AM ----------------------> 12:00PM

Suppose the database crashed at 8:00AM, so we have backed up data at 6:00 AM thus we can recover data till 6:00 AM , however
the database crashed at 8:00 AM thus till be need 2 hr recorder data and this recorder data can be found in replication logs.

3) These can be used in changed streams or changed data capture, lets suppose as soon as a new user register in our system we need to send him an welcome email or sms. Now there can be multiple ways of doing this but however one another simple way is as soon as a user will register so the primary user table will get updated and it will log it in replication logs. There can be another process running
when sees the replication logs and sees the user table updating with a new user can take care of sending this welcome email or sms.


Now there can arise a problem in which nodes fail. So there can two scenarios:

a) Secondary fails
b) Primary or master fails

If Secondary fails then its a simple problem of adding a new secondary:

Now lets understand how we can add a secondary or a master to a database cluster. 
We will simply take a snapshot of primary and an replication iD. We can get the replication id from the replication logs for the time stamp at
which we took snap shot of the primary. Now we will copy this snap shot to secondary, suppose it took
4 hours and in these 4 hours the primary may have changed by attending multiple queries and must had logged it in replication logs,
we had replication Id of the time at which primary snap shot was taken, so all replication ID's in replication logs after that
ID are new entries by primary and its where the data now differs between primary and new second. We connect new secondary into the
network and connect it with primary and it updates itself after that replication ID which it has stored.


However if a primary fails then this problem can be tackled via election.

**Election**

Say we have a database cluster with 1 primary P1 and two secondary S1 & S2. Now P1 has failed. All these nodes communicate with 
each other regarding the health and other things. Say S1 got to know that P1 has failed so every node has a replication ID associated
with them which states the current replication id by what they have cloned from primary through replication logs.

Now say replication id of primary is 10 as replication logs has total 10 entries, whereas replication ID of S1 is 9 and replication id
of S2 is 8. Now based on these replication id's since s1 is more nearer to primary and thus via voting s1 has 2/3 votes to be a primary
i,e one by s1 and one by s2 . Primary cant vote as its dead.

So by majority i,e elections S1 takes up primary role.

Note:

**Why we generally have odd number of nodes in a database cluster**

The two main reason behind having odd number of nodes in a database cluster is 
a) Cost effective
b) Greater fault tolerance.

Lets understand them suppose I have 2 clusters with 3 and 4 devices respectively

Cluster1: 3 nodes
                                   
     Primary
   |         |
   S1        S2


Cluster2: 4 nodes

    Primary
       |
       S1
    |      |
    S2     S3


Now lets fault tolerance in them one by one.

In cluster 1, if primary fails then S1 can have max votes as being primary via election is 2/3 (both S1&S2 voted for S1)
and S2 can also have max votes as being primary via election is (2/3) (both s1 & S2 voted for S2).

Lets talk about max votes which could exist in the system if primary was also up, there were 3 nodes, so majority is atleast 2
i,e if someone gets 2 or more than 2 then they win.

Now in case of P1 failure max votes s1 or s2 can get is 2 as explained which is greater than or equal to majority thus system can keep
running.

Now lets suppose than S1 also failed along with Primary so max votes with which S2 can become primary is 1 i,e vote of itself only which is not equal or greater than majority.

and thus if S1 and primary both fails the system can crash as S2 can't be a primary as it don't have majority votes thus 
in order to keep system running you can afford the loss of only one node and thus fault tolerance of this system is 1 only.

Now lets talk about cluster with 4 devices, clearly it has greater nodes and every node needs some CPU computation, RAM etc
which comes with code thus cluster 2 is more costlier than cluster 1.

Now if the system is up and running without any node failing, the total votes are 4.
At any time majority votes are greater than 2. Not 2 as it can lead to a tie.

Now say P1 fails:

S1 can have max 3 votes(vote of s2, s3 and itself)
S2 can have max 3 votes (vote of S1,S3 and itself)
S3 can have max 3 votes (vote of S1,S2 and itself)

Thus 3 is sufficient for majority and thus system can keep running.

Now if say P1 and S2 both fails now

S2 can have max 2 votes (S3 and itself)
S3 can have max 2 votes(S2 and itself)

2 votes are less than majority so no primary can be selected and in this case also the system can afford the loss
of only one node after that the system can't pick primary and crashes. So fault tolerance is still 1 only.

Thus an odd node system is better than even node system.

There are some problems associated with Leader Follower topology, since the secondary gets updated via replication logs
thus there can be a replication latency and suppose the write is performed by primary is not yet replicated by the slaves or
secondaries and a request came to query a user for its data, then it can lead to data abnormality.

Database like Mongo follow this, single leader and follower topology.


**b) Multi leader follower**

Now in case of multi leader follower there is no one primary rather we have multiple different primary.
These every primary can have their own secondaries. These primaries keep other primaries updated and thus
corresponding respective slaves belonging to those primaries will also be updated.

Consider some use cases:

Lets take an example suppose I have a global distributed system which has users in US as well as India.

Now we have created separate data centers say one in US and one in India.
Now US data center has its own database cluster with a primary and its secondary and similarly Indian data center also has
its own database cluster with primary and its secondary.

Both of them can operate independently and every user of US will be routed to US data center only whereas every Indian
user will be routed to Indian data center only. Now these master nodes present at both locations are inter connected to each other and is
responsible for keeping the data clusters synchronized to each other, then they send them updates and once updates reaches master they can be
then further taken to the slaves of individual data centers. 

Such type of arrangement is multi leader follower.

There can be problems with such arrangements:

1) Say a US citizen now traveled to India so while accessing the application from India the US person will be routed
   to US data centers only and thus there can be a little latency.

2) Suppose we want the masters or primary of each data center to be connected to each other so that they share updates
   of each other regularly so that both are in sync meaning Indian data center also has US data and US data center also has
   Indian data, so there can be a lot of latency in replication, moreover can however also gives rise to conflicts in data.

   Lets understand how conflict can happen with help of some example.

   So in this scenario our requirement is that both US data center and Indian data center are in sync to each other,
   A us person updated in profile so it got updated in US data center, before syncing of this info would have happened
   with Indian data center he somehow say through VPN connected to Indian database and updated his profile with some different
   info, now while syncing of these two databases, which data would be considered final and thus there is a conflict.

**Ways to solve conflicts**

There can be multiple ways to solve this conflict of data

a) Different routing of data like US data to US data center and Indian data to Indian data center without 
   syncing of these two clusters.

b) Last one wins:
   This approach is simple, every write is associated with a time stamp and the latest or last one goes through. However in this
   all datacenter should sync their clocks to UTC using a protocol like NTP (Network Time Protocol).

c) Auto merging:
    Classic example is github, if no specific one entry or file is being changed by more than 1 primary than simply do auto
    merge else ask for manual merge

d) Manual merge just like we do in git or any version control.

Another use case for multi leader follower can be:

Suppose we have made an arrangement that any user making change will be stored in his machine only i,e local database and 
then the global cluster can take updates from the local machine's database. Similarly with n users.

So every machine's local act as a primary and the global cluster is also a primary and these primary has to communicate 
with each other such that global gets in sync.

This can have a problem like if the client's machine goes offline than there is no way for global's cluster primary to 
get in sync however it can provide seamless experience to client as there will be little to no latency.



2) **Leader Less replication**

As in leader led replication, there is no leader present in this database cluster and all nodes are considered same. Cassandra uses this. Lets see how this works.
Suppose a client sends a request and in our database cluster we have 3 nodes. So there will be a coordinator, this can be within the database
cluster or even on server. Say its on server, now say a read query comes in , so server propagates this read query too all the nodes in database cluster
Now there exists a configuration which is handled by coordinator, this configuration has 

Read threshold -> R
Write threshold -> W

Read threshold means that in case of a read query how many different node should respond with data.
Write threshold means that in case of a write query how many different nodes should acknowledge it and updated themselves.

Say a read query comes in and we have 3 nodes in Database cluster (N) so if R(read threshold is considered 2) then if two nodes responded back than only
the coordinator will respond back to client with the result of query.

Say a write query comes in and if W( write threshold is set as 2) then only after 2 nodes are done updating the data then the coordinator will send
response OK to client.

This is called Quorum 
i,e read Threshold, write threshold and nodes such that W,R <= N

Now its very important to always maintain majority quorum. 

Majority quorum is W + R > N where (W,R =< N) i,e write threshold + read threshold > total number of nodes in cluster.

Lets understand why its important.
We have N=3, W=2 and R=2

So we have 3 nodes 


                       Node1                Node2               Node3
                       (k=2)               (k=1)                (k=2)

So a write request comes which write a value for k=2 in nodes. Since write threshold was 2 so the coordinator waited until 2 nodes updated itself
and then responded back the client. So say node 1 and node 3 successfully updated its value of k however node 2 still has outdated value.

Now our read threshold is 2. Now a read request comes in for value of k, since the read threshold is set to 2 so until 2 nodes responds back the coordinator will not respond back to the client, so which all 2 nodes can respond back node1,node2 or node1,node3 or node2,node3, in all the cases the updated value k will be sent
to coordinator and he will respond back to client with updated value.

Clearly via such arrangement we always ensure correctness in our queries and this we can ensure using majority quorum configuration i,e
W + R > N where (W,R =< N) is a majority Quorum.

Now but one problem still remains i,e Node 2 is still outdated and how can it be updated . So there are two ways to do it, i,e to maintain consistency
in all the nodes.

**a) Read repair:**

Now we saw that above the coordinator can receive an outdated value for node 2 in any of the read so it will know that node 2 is not updated and thus
simply it will issue an write query to node 2 to update its value with the updated value.

**b) Anti entropy process**

Anti entropy process is a background process which happens to keep updating data as it checks for differences and updates them.
Its best suited for data which is not read consistently and is prone to some latency as this updation takes time.
